import asyncio
import json
from scrapers.deepseek import DeepSeekScraper
from scrapers.gemini import GeminiScraper  # æ–°å¢
from core.analyser import CTFAnalyser
from core.archiver import CTFArchiver
from loguru import logger

async def process_ctf_link(url: str):
    # 1. è·¯ç”±åˆ†é…
    if "deepseek.com" in url:
        scraper = DeepSeekScraper()
    elif "google.com" in url or "aistudio" in url:
        scraper = GeminiScraper()
    else:
        logger.error(f"æš‚ä¸æ”¯æŒè¯¥å¹³å°: {url}")
        return

    # 2. æŠ“å–åŸå§‹æ•°æ®
    raw_data = await scraper.scrape_share_link(url)
    if not raw_data: 
        logger.error(f"æ— æ³•ä»è¯¥é“¾æ¥è·å–å†…å®¹: {url}")
        return

    # 3. ä¿å­˜ä¸´æ—¶æ–‡ä»¶ä¾›åˆ†æ
    temp_raw = "temp_raw.json"
    with open(temp_raw, "w", encoding="utf-8") as f:
        json.dump(raw_data, f, ensure_ascii=False)
        
    # 4. AI è¯­ä¹‰åˆ†æ
    analyser = CTFAnalyser()
    structured_result = await analyser.analyse_file(temp_raw)
    
    if not structured_result:
        logger.error("AI åˆ†æå¤±è´¥ï¼Œå¯èƒ½æ˜¯æŠ“å–çš„å†…å®¹ä¸åŒ…å«æœ‰æ•ˆçš„å¯¹è¯ã€‚")
        return

    # 5. å½’æ¡£ (è¿™é‡Œç°åœ¨åŒ¹é…äº†ä¸Šé¢æ–°å¢çš„ archive_data)
    archiver = CTFArchiver()
    final_path = archiver.archive_data(structured_result)
    
    if final_path:
        logger.success(f"ğŸ‰ å½’æ¡£æˆåŠŸ: {final_path}")
    else:
        logger.error("å½’æ¡£å¤±è´¥ã€‚")

async def main():
    urls = [
        "https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221AVkc4dc5ES2tNMPQW9sEyrFwzICs1cBg%22%5D,%22action%22:%22open%22,%22userId%22:%22105912647283914331320%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing"
    ]
    
    for url in urls:
        await process_ctf_link(url)

if __name__ == "__main__":
    asyncio.run(main())